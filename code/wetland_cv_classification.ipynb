{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "83Lz8l70O-hd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6af210f9-75f5-4790-dc38-a6448a7bc1cb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated triple-quoted string literal (detected at line 1061) (ipython-input-3157952848.py, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3157952848.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 1061)\n"
          ]
        }
      ],
      "source": [
        "================================================================================\n",
        "WETLAND PERMANENCE CLASSIFICATION - FINAL CORRECTED VERSION\n",
        "================================================================================\n",
        "Brookings County, South Dakota - 2024\n",
        "Method: NDWI Coefficient of Variation Classification\n",
        "Author: [Image Bhattarai]\n",
        "Date: November 2024\n",
        "\n",
        "\n",
        "- Fixed CDL codes (using only 111, 190, 195)\n",
        "- Added edge buffering (2-pixel erosion) to reduce mixed pixel effects\n",
        "- Improved patch counting (4-connectivity)\n",
        "- Added data validation and quality checks\n",
        "- Exports: Classification GeoTIFF, CV GeoTIFF, Excel workbook\n",
        "- Enhanced error handling and reporting\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns\n",
        "from scipy.ndimage import label, find_objects, binary_erosion\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "from rasterio.warp import reproject, Resampling\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WETLAND PERMANENCE CLASSIFICATION - FINAL CORRECTED VERSION\")\n",
        "print(\"Brookings County, South Dakota - 2024\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/GIS-project/Colab_8Class_Unsupervised_Outputs/01_Mosaicked_Indices/\"\n",
        "cdl_path = \"/content/drive/MyDrive/cdl_2024_brookings.tif\"\n",
        "output_path = \"/content/drive/MyDrive/Wetland_Report_Final_Corrected/\"\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "print(f\"\\n‚úì Output directory: {output_path}\")\n",
        "\n",
        "# Analysis parameters\n",
        "LOW_CV_THRESHOLD = 20\n",
        "HIGH_CV_THRESHOLD = 50\n",
        "MIN_PATCH_SIZE_HA = 0.01\n",
        "EDGE_BUFFER_PIXELS = 2  # 2-pixel erosion to remove edge effects\n",
        "\n",
        "print(f\"\\nAnalysis Parameters:\")\n",
        "print(f\"  CV Thresholds: <{LOW_CV_THRESHOLD}% (Permanent), {LOW_CV_THRESHOLD}-{HIGH_CV_THRESHOLD}% (Vegetated), >{HIGH_CV_THRESHOLD}% (Seasonal)\")\n",
        "print(f\"  Minimum patch size: {MIN_PATCH_SIZE_HA} ha\")\n",
        "print(f\"  Edge buffer: {EDGE_BUFFER_PIXELS} pixels\")\n",
        "\n",
        "seasons = ['Spring', 'Summer', 'Fall']\n",
        "ndwi_files = {\n",
        "    'Spring': f\"{base_path}NDWI_Spring_Full_10m.tif\",\n",
        "    'Summer': f\"{base_path}NDWI_Summer_Full_10m.tif\",\n",
        "    'Fall': f\"{base_path}NDWI_Fall_Full_10m.tif\"\n",
        "}\n",
        "\n",
        "class_names = {\n",
        "    1: 'Permanent Wetland',\n",
        "    2: 'Vegetated Wetland',\n",
        "    3: 'Seasonal Wetland'\n",
        "}\n",
        "\n",
        "class_names_full = {\n",
        "    1: 'Permanent Wetland (Open Water)',\n",
        "    2: 'Vegetated Wetland (Emergent)',\n",
        "    3: 'Seasonal Wetland (Ephemeral)'\n",
        "}\n",
        "\n",
        "# Visualization colors\n",
        "colors_map = ['#4169E1', '#FFA500', '#FF4500']  # Blue, Orange, Red\n",
        "colors_pie = ['#4169E1', '#FFA500', '#FF4500']\n",
        "colors_lines = ['#4169E1', '#FFA500', '#FF4500']\n",
        "markers = ['o', '^', 's']\n",
        "\n",
        "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8, edgecolor='black', linewidth=2)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD AND VALIDATE DATA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: LOADING AND VALIDATING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "data = {'NDWI': {}}\n",
        "metadata = None\n",
        "\n",
        "# Load NDWI data and get target grid\n",
        "try:\n",
        "    with rasterio.open(ndwi_files['Spring']) as src:\n",
        "        target_transform = src.transform\n",
        "        target_width = src.width\n",
        "        target_height = src.height\n",
        "        target_crs = src.crs\n",
        "        metadata = src.meta.copy()\n",
        "        pixel_size = abs(src.transform[0])\n",
        "\n",
        "    print(f\"\\nTarget Grid (from NDWI):\")\n",
        "    print(f\"  Dimensions: {target_width} x {target_height} pixels\")\n",
        "    print(f\"  Resolution: {pixel_size:.1f}m\")\n",
        "    print(f\"  CRS: {target_crs}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading NDWI reference file: {e}\")\n",
        "    raise\n",
        "\n",
        "# Load all NDWI layers\n",
        "print(f\"\\nLoading NDWI data for {len(seasons)} seasons...\")\n",
        "for season, file_path in ndwi_files.items():\n",
        "    try:\n",
        "        with rasterio.open(file_path) as src:\n",
        "            ndwi_data = src.read(1).astype(np.float32)\n",
        "            ndwi_data[ndwi_data == src.nodata] = np.nan\n",
        "            data['NDWI'][season] = ndwi_data\n",
        "\n",
        "            # Validate NDWI range\n",
        "            valid_data = ndwi_data[~np.isnan(ndwi_data)]\n",
        "            min_val, max_val = np.min(valid_data), np.max(valid_data)\n",
        "            print(f\"  {season:8s}: Range [{min_val:+.3f}, {max_val:+.3f}] | Valid pixels: {len(valid_data):,}\")\n",
        "\n",
        "            if min_val < -1.5 or max_val > 1.5:\n",
        "                print(f\"    ‚ö†Ô∏è  WARNING: NDWI values outside expected range [-1, +1]\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR loading {season} NDWI: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"‚úì All NDWI data loaded successfully\")\n",
        "\n",
        "# Load CDL\n",
        "print(f\"\\nLoading CDL 2024...\")\n",
        "try:\n",
        "    with rasterio.open(cdl_path) as src:\n",
        "        cdl_30m = src.read(1)\n",
        "        cdl_transform = src.transform\n",
        "        cdl_crs = src.crs\n",
        "        cdl_res = abs(src.transform[0])\n",
        "\n",
        "    print(f\"  Dimensions: {cdl_30m.shape[1]} x {cdl_30m.shape[0]} pixels\")\n",
        "    print(f\"  Resolution: {cdl_res:.1f}m\")\n",
        "\n",
        "    # CRITICAL: Check what CDL codes are actually present\n",
        "    unique_codes = np.unique(cdl_30m)\n",
        "    print(f\"\\n  CDL codes present in file: {len(unique_codes)} unique values\")\n",
        "\n",
        "    # Check for wetland codes\n",
        "    MODERN_WETLAND_CODES = {\n",
        "        111: 'Open Water',\n",
        "        190: 'Woody Wetlands',\n",
        "        195: 'Herbaceous Wetlands'\n",
        "    }\n",
        "\n",
        "    DEPRECATED_CODES = {\n",
        "        83: 'Water (deprecated)',\n",
        "        87: 'Wetlands (deprecated)'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n  Wetland code analysis:\")\n",
        "    wetland_found = False\n",
        "    for code, name in MODERN_WETLAND_CODES.items():\n",
        "        count = np.sum(cdl_30m == code)\n",
        "        if count > 0:\n",
        "            print(f\"    ‚úì Code {code} ({name}): {count:,} pixels\")\n",
        "            wetland_found = True\n",
        "        else:\n",
        "            print(f\"    ‚úó Code {code} ({name}): NOT FOUND\")\n",
        "\n",
        "    for code, name in DEPRECATED_CODES.items():\n",
        "        count = np.sum(cdl_30m == code)\n",
        "        if count > 0:\n",
        "            print(f\"    ‚ö†Ô∏è  Code {code} ({name}): {count:,} pixels (SHOULD NOT BE USED)\")\n",
        "\n",
        "    if not wetland_found:\n",
        "        print(f\"\\n    ‚ùå ERROR: No wetland codes found in CDL file!\")\n",
        "        print(f\"    First 20 unique codes in file: {unique_codes[:20]}\")\n",
        "        raise ValueError(\"No wetland pixels detected in CDL\")\n",
        "\n",
        "    print(\"‚úì CDL loaded successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading CDL: {e}\")\n",
        "    raise\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: CREATE AND ALIGN WETLAND MASK (FIXED)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: CREATING AND ALIGNING WETLAND MASK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use ONLY modern CDL codes (111, 190, 195)\n",
        "WETLAND_CODES = {\n",
        "    111: 'Open Water',\n",
        "    190: 'Woody Wetlands',\n",
        "    195: 'Herbaceous Wetlands'\n",
        "}\n",
        "\n",
        "print(f\"Creating wetland mask using CDL codes: {list(WETLAND_CODES.keys())}\")\n",
        "\n",
        "# Create mask at 30m resolution\n",
        "wetland_mask_30m = np.isin(cdl_30m, list(WETLAND_CODES.keys()))\n",
        "wetland_pixels_30m = np.sum(wetland_mask_30m)\n",
        "area_30m = wetland_pixels_30m * (cdl_res**2) / 10000  # Convert to ha\n",
        "\n",
        "print(f\"\\nWetland mask at 30m resolution:\")\n",
        "print(f\"  Pixels: {wetland_pixels_30m:,}\")\n",
        "print(f\"  Area: {area_30m:.2f} ha\")\n",
        "\n",
        "# Reproject to 10m grid\n",
        "print(f\"\\nReprojecting from 30m to 10m resolution...\")\n",
        "wetland_mask_10m = np.zeros((target_height, target_width), dtype=np.uint8)\n",
        "\n",
        "reproject(\n",
        "    source=wetland_mask_30m.astype(np.uint8),\n",
        "    destination=wetland_mask_10m,\n",
        "    src_transform=cdl_transform,\n",
        "    src_crs=cdl_crs,\n",
        "    dst_transform=target_transform,\n",
        "    dst_crs=target_crs,\n",
        "    resampling=Resampling.nearest,\n",
        "    dst_nodata=0\n",
        ")\n",
        "\n",
        "wetland_mask_10m = wetland_mask_10m.astype(bool)\n",
        "pixel_area_ha = (pixel_size**2) / 10000  # 10m x 10m = 100 m¬≤ = 0.01 ha\n",
        "\n",
        "print(f\"‚úì Reprojection complete\")\n",
        "print(f\"\\nWetland mask at 10m resolution:\")\n",
        "print(f\"  Pixels: {np.sum(wetland_mask_10m):,}\")\n",
        "print(f\"  Area: {np.sum(wetland_mask_10m) * pixel_area_ha:.2f} ha\")\n",
        "print(f\"  Pixel size: {pixel_area_ha:.4f} ha\")\n",
        "\n",
        "# Apply edge buffering to reduce mixed pixel effects\n",
        "print(f\"\\nApplying {EDGE_BUFFER_PIXELS}-pixel erosion to remove edge effects...\")\n",
        "wetland_mask_original = wetland_mask_10m.copy()\n",
        "structure = np.ones((3, 3))\n",
        "wetland_mask_buffered = binary_erosion(wetland_mask_10m, structure=structure, iterations=EDGE_BUFFER_PIXELS)\n",
        "\n",
        "pixels_removed = np.sum(wetland_mask_original) - np.sum(wetland_mask_buffered)\n",
        "area_removed = pixels_removed * pixel_area_ha\n",
        "\n",
        "print(f\"  Pixels removed (edge effects): {pixels_removed:,}\")\n",
        "print(f\"  Area removed: {area_removed:.2f} ha ({100*pixels_removed/np.sum(wetland_mask_original):.1f}%)\")\n",
        "print(f\"  Final core wetland area: {np.sum(wetland_mask_buffered) * pixel_area_ha:.2f} ha\")\n",
        "\n",
        "# Use buffered mask for analysis\n",
        "wetland_mask = wetland_mask_buffered\n",
        "total_wetland_pixels = np.sum(wetland_mask)\n",
        "total_wetland_area = total_wetland_pixels * pixel_area_ha\n",
        "\n",
        "print(f\"\\n‚úì Final wetland mask created\")\n",
        "print(f\"  Total pixels: {total_wetland_pixels:,}\")\n",
        "print(f\"  Total area: {total_wetland_area:.2f} ha\")\n",
        "\n",
        "# --- Figure 1: Wetland Mask ---\n",
        "print(\"\\nüìä Generating Figure 1: Wetland Mask...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "wetland_display = wetland_mask.astype(float)\n",
        "wetland_display[~wetland_mask] = np.nan\n",
        "ax.imshow(wetland_display, cmap='Blues', interpolation='nearest')\n",
        "ax.set_title('Figure 1: Wetland Mask - Brookings County, SD\\n(USDA CDL 2024, 2-pixel edge buffer applied)',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.axis('off')\n",
        "textstr = (f'Total Area: {total_wetland_area:.2f} ha\\n'\n",
        "           f'Resolution: 10m\\n'\n",
        "           f'CDL Codes: 111, 190, 195\\n'\n",
        "           f'Edge Buffer: {EDGE_BUFFER_PIXELS} pixels')\n",
        "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=12,\n",
        "        verticalalignment='top', bbox=props, family='monospace')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure1_Wetland_Mask.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure1_Wetland_Mask.png\")\n",
        "\n",
        "# --- Export Wetland Mask as GeoTIFF ---\n",
        "print(\"\\nüíæ Exporting Wetland Mask as GeoTIFF...\")\n",
        "mask_meta = metadata.copy()\n",
        "mask_meta.update({'dtype': 'uint8', 'nodata': 0, 'count': 1})\n",
        "mask_export = wetland_mask.astype(np.uint8)\n",
        "# Set non-wetland pixels to nodata (0)\n",
        "mask_export[~wetland_mask] = 0\n",
        "\n",
        "with rasterio.open(f'{output_path}Wetland_Mask.tif', 'w', **mask_meta) as dst:\n",
        "    dst.write(mask_export, 1)\n",
        "print(\"‚úì Saved: Wetland_Mask.tif\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: CALCULATE COEFFICIENT OF VARIATION (CV)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: CALCULATING COEFFICIENT OF VARIATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Stack seasonal NDWI\n",
        "ndwi_stack = np.stack([data['NDWI'][s] for s in seasons], axis=0)\n",
        "\n",
        "# Calculate statistics\n",
        "mean_ndwi = np.nanmean(ndwi_stack, axis=0)\n",
        "std_ndwi = np.nanstd(ndwi_stack, axis=0)\n",
        "\n",
        "# CV = (std / |mean|) * 100\n",
        "# Use absolute value to handle negative NDWI (Gao index)\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    cv_map = np.divide(std_ndwi, np.abs(mean_ndwi),\n",
        "                       out=np.full_like(mean_ndwi, np.nan),\n",
        "                       where=(mean_ndwi != 0)) * 100\n",
        "\n",
        "# Apply wetland mask\n",
        "cv_wetlands = cv_map.copy()\n",
        "cv_wetlands[~wetland_mask] = np.nan\n",
        "\n",
        "# Calculate CV statistics\n",
        "cv_valid = cv_wetlands[~np.isnan(cv_wetlands)]\n",
        "print(f\"\\nCV Statistics (wetland pixels only):\")\n",
        "print(f\"  Valid pixels: {len(cv_valid):,}\")\n",
        "print(f\"  Min CV: {np.min(cv_valid):.1f}%\")\n",
        "print(f\"  Max CV: {np.max(cv_valid):.1f}%\")\n",
        "print(f\"  Mean CV: {np.mean(cv_valid):.1f}%\")\n",
        "print(f\"  Median CV: {np.median(cv_valid):.1f}%\")\n",
        "print(f\"  25th percentile: {np.percentile(cv_valid, 25):.1f}%\")\n",
        "print(f\"  75th percentile: {np.percentile(cv_valid, 75):.1f}%\")\n",
        "\n",
        "print(\"‚úì CV calculated successfully\")\n",
        "\n",
        "# --- Figure 2: CV Map ---\n",
        "print(\"\\nüìä Generating Figure 2: CV Map...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "cv_colors = ['#006837', '#a6d96a', '#ffffbf', '#fdae61', '#d7191c']\n",
        "cmap_cv = mcolors.LinearSegmentedColormap.from_list('cv_cmap', cv_colors, N=256)\n",
        "im = ax.imshow(cv_wetlands, cmap=cmap_cv, vmin=0, vmax=100, interpolation='bilinear')\n",
        "cbar = plt.colorbar(im, ax=ax, fraction=0.03, pad=0.04, extend='max')\n",
        "cbar.set_label('Coefficient of Variation (%)', fontsize=14, fontweight='bold')\n",
        "cbar.ax.hlines([LOW_CV_THRESHOLD, HIGH_CV_THRESHOLD], 0, 1, colors='black', linewidth=2, linestyles='--')\n",
        "ax.set_title('Figure 2: NDWI Coefficient of Variation (CV) Map\\nBrookings County, SD',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.axis('off')\n",
        "textstr = (f'CV Statistics:\\n'\n",
        "           f'Mean: {np.mean(cv_valid):.1f}%\\n'\n",
        "           f'Median: {np.median(cv_valid):.1f}%\\n'\n",
        "           f'Range: [{np.min(cv_valid):.1f}%, {np.max(cv_valid):.1f}%]\\n'\n",
        "           f'Valid Pixels: {len(cv_valid):,}')\n",
        "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='top', bbox=props, family='monospace')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure2_CV_Map.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure2_CV_Map.png\")\n",
        "\n",
        "# Export CV as GeoTIFF\n",
        "print(\"\\nüíæ Exporting CV map as GeoTIFF...\")\n",
        "cv_meta = metadata.copy()\n",
        "cv_meta.update({'dtype': 'float32', 'nodata': -9999, 'count': 1})\n",
        "cv_export = cv_map.copy()\n",
        "cv_export[np.isnan(cv_export)] = -9999\n",
        "with rasterio.open(f'{output_path}CV_Map.tif', 'w', **cv_meta) as dst:\n",
        "    dst.write(cv_export.astype('float32'), 1)\n",
        "print(\"‚úì Saved: CV_Map.tif\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: CLASSIFY WETLAND PERMANENCE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: CLASSIFYING WETLAND PERMANENCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Apply thresholds\n",
        "classification = np.full_like(cv_wetlands, np.nan)\n",
        "classification[cv_wetlands < LOW_CV_THRESHOLD] = 1  # Permanent\n",
        "classification[(cv_wetlands >= LOW_CV_THRESHOLD) & (cv_wetlands < HIGH_CV_THRESHOLD)] = 2 # Vegetated\n",
        "classification[cv_wetlands >= HIGH_CV_THRESHOLD] = 3      # Seasonal\n",
        "\n",
        "# Initial classification counts\n",
        "for class_val in [1, 2, 3]:\n",
        "    count = np.sum(classification == class_val)\n",
        "    print(f\"  {class_names[class_val]:20s}: {count:,} pixels ({count * pixel_area_ha:.2f} ha)\")\n",
        "\n",
        "print(\"\\nüßπ Cleaning classification (removing patches < {MIN_PATCH_SIZE_HA} ha)...\")\n",
        "classification_clean = classification.copy()\n",
        "\n",
        "# Use 4-connectivity for more conservative patch counting\n",
        "structure_4conn = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
        "\n",
        "for class_val in [1, 2, 3]:\n",
        "    class_mask = (classification == class_val)\n",
        "    labeled, num_features = label(class_mask, structure=structure_4conn)\n",
        "\n",
        "    patches_removed = 0\n",
        "    area_removed = 0\n",
        "\n",
        "    for patch_id in range(1, num_features + 1):\n",
        "        patch_pixels = np.sum(labeled == patch_id)\n",
        "        patch_area = patch_pixels * pixel_area_ha\n",
        "\n",
        "        if patch_area < MIN_PATCH_SIZE_HA:\n",
        "            classification_clean[labeled == patch_id] = np.nan\n",
        "            patches_removed += 1\n",
        "            area_removed += patch_area\n",
        "\n",
        "    print(f\"  {class_names[class_val]:20s}: Removed {patches_removed:,} patches ({area_removed:.2f} ha)\")\n",
        "\n",
        "print(\"‚úì Classification complete\")\n",
        "\n",
        "# Final classification counts\n",
        "total_classified_pixels = 0\n",
        "print(\"\\nFinal classification:\")\n",
        "for class_val in [1, 2, 3]:\n",
        "    count = np.sum(classification_clean == class_val)\n",
        "    total_classified_pixels += count\n",
        "    area = count * pixel_area_ha\n",
        "    print(f\"  {class_names[class_val]:20s}: {count:,} pixels ({area:.2f} ha)\")\n",
        "\n",
        "print(f\"  {'TOTAL':20s}: {total_classified_pixels:,} pixels ({total_classified_pixels * pixel_area_ha:.2f} ha)\")\n",
        "\n",
        "# --- Figure 3: Classification Map ---\n",
        "print(\"\\nüìä Generating Figure 3: Classification Map...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "class_display = np.full(classification_clean.shape + (3,), 0.95)\n",
        "class_display[classification_clean == 1] = mcolors.to_rgb(colors_map[0])\n",
        "class_display[classification_clean == 2] = mcolors.to_rgb(colors_map[1])\n",
        "class_display[classification_clean == 3] = mcolors.to_rgb(colors_map[2])\n",
        "\n",
        "ax.imshow(class_display, interpolation='nearest')\n",
        "ax.set_title('Figure 3: Wetland Permanence Classification Map\\nBrookings County, SD (NDWI CV Method)',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.axis('off')\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor=colors_map[0], edgecolor='black',\n",
        "          label=f'{class_names_full[1]} (CV < {LOW_CV_THRESHOLD}%)'),\n",
        "    Patch(facecolor=colors_map[1], edgecolor='black',\n",
        "          label=f'{class_names_full[2]} (CV {LOW_CV_THRESHOLD}-{HIGH_CV_THRESHOLD}%)'),\n",
        "    Patch(facecolor=colors_map[2], edgecolor='black',\n",
        "          label=f'{class_names_full[3]} (CV > {HIGH_CV_THRESHOLD}%)')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=11, framealpha=0.95,\n",
        "         edgecolor='black', fancybox=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure3_Classification_Map.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure3_Classification_Map.png\")\n",
        "\n",
        "# Export classification as GeoTIFF\n",
        "print(\"\\nüíæ Exporting classification as GeoTIFF...\")\n",
        "class_meta = metadata.copy()\n",
        "class_meta.update({'dtype': 'uint8', 'nodata': 0, 'count': 1})\n",
        "class_export = classification_clean.copy()\n",
        "class_export[np.isnan(class_export)] = 0\n",
        "with rasterio.open(f'{output_path}Wetland_Classification.tif', 'w', **class_meta) as dst:\n",
        "    dst.write(class_export.astype('uint8'), 1)\n",
        "    # Write class names as metadata\n",
        "    dst.update_tags(1,\n",
        "                   class_1='Permanent Wetland',\n",
        "                   class_2='Vegetated Wetland',\n",
        "                   class_3='Seasonal Wetland')\n",
        "print(\"‚úì Saved: Wetland_Classification.tif\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: CALCULATE COMPREHENSIVE STATISTICS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: CALCULATING STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Area-based statistics\n",
        "area_stats = {}\n",
        "for class_val in [1, 2, 3]:\n",
        "    count = np.sum(classification_clean == class_val)\n",
        "    area_ha = count * pixel_area_ha\n",
        "    pct = (count / total_classified_pixels) * 100\n",
        "    area_stats[class_val] = {'count': count, 'area_ha': area_ha, 'pct': pct}\n",
        "\n",
        "# Patch-based statistics (using 4-connectivity)\n",
        "patch_stats = {}\n",
        "for class_val in [1, 2, 3]:\n",
        "    class_mask = (classification_clean == class_val)\n",
        "    labeled_array, num_patches = label(class_mask, structure=structure_4conn)\n",
        "\n",
        "    patch_sizes = []\n",
        "    for patch_id in range(1, num_patches + 1):\n",
        "        patch_pixels = np.sum(labeled_array == patch_id)\n",
        "        patch_area_ha = patch_pixels * pixel_area_ha\n",
        "        if patch_area_ha >= MIN_PATCH_SIZE_HA:\n",
        "            patch_sizes.append(patch_area_ha)\n",
        "\n",
        "    patch_stats[class_val] = {\n",
        "        'num_patches': len(patch_sizes),\n",
        "        'sizes': patch_sizes,\n",
        "        'mean_size': np.mean(patch_sizes) if patch_sizes else 0,\n",
        "        'median_size': np.median(patch_sizes) if patch_sizes else 0,\n",
        "        'std_size': np.std(patch_sizes) if patch_sizes else 0,\n",
        "        'min_size': np.min(patch_sizes) if patch_sizes else 0,\n",
        "        'max_size': np.max(patch_sizes) if patch_sizes else 0\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{class_names[class_val]}:\")\n",
        "    print(f\"  Number of patches: {len(patch_sizes):,}\")\n",
        "    print(f\"  Mean size: {patch_stats[class_val]['mean_size']:.4f} ha\")\n",
        "    print(f\"  Median size: {patch_stats[class_val]['median_size']:.4f} ha\")\n",
        "    print(f\"  Size range: [{patch_stats[class_val]['min_size']:.4f}, {patch_stats[class_val]['max_size']:.2f}] ha\")\n",
        "\n",
        "total_patches = sum([patch_stats[i]['num_patches'] for i in [1, 2, 3]])\n",
        "print(f\"\\n‚úì Total patches: {total_patches:,}\")\n",
        "\n",
        "# CV statistics by class\n",
        "cv_stats_by_class = {}\n",
        "for class_val in [1, 2, 3]:\n",
        "    class_cv = cv_wetlands[classification_clean == class_val]\n",
        "    class_cv = class_cv[~np.isnan(class_cv)]\n",
        "\n",
        "    cv_stats_by_class[class_val] = {\n",
        "        'mean': np.mean(class_cv),\n",
        "        'median': np.median(class_cv),\n",
        "        'std': np.std(class_cv),\n",
        "        'min': np.min(class_cv),\n",
        "        'max': np.max(class_cv),\n",
        "        'q25': np.percentile(class_cv, 25),\n",
        "        'q75': np.percentile(class_cv, 75)\n",
        "    }\n",
        "\n",
        "# --- Figure 4: Pie Charts ---\n",
        "print(\"\\nüìä Generating Figure 4: Pie Charts...\")\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Area distribution\n",
        "sizes_area = [area_stats[i]['area_ha'] for i in [1, 2, 3]]\n",
        "labels_area = [f\"{class_names[i]}\\n{area_stats[i]['pct']:.1f}%\" for i in [1, 2, 3]]\n",
        "ax1.pie(sizes_area, labels=labels_area, colors=colors_pie, startangle=90,\n",
        "       autopct='', textprops={'fontsize': 12, 'weight': 'bold'})\n",
        "ax1.set_title(f'Wetland Area Distribution\\nTotal: {total_classified_pixels * pixel_area_ha:.2f} ha',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "# Patch distribution\n",
        "sizes_patch = [patch_stats[i]['num_patches'] for i in [1, 2, 3]]\n",
        "labels_patch = [f\"{class_names[i]}\\n{patch_stats[i]['num_patches']/total_patches*100:.1f}%\" for i in [1, 2, 3]]\n",
        "ax2.pie(sizes_patch, labels=labels_patch, colors=colors_pie, startangle=90,\n",
        "       autopct='', textprops={'fontsize': 12, 'weight': 'bold'})\n",
        "ax2.set_title(f'Wetland Patch Count Distribution\\nTotal: {total_patches:,} patches',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure4_Pie_Charts.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure4_Pie_Charts.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: SEASONAL FINGERPRINT VALIDATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: SEASONAL FINGERPRINT VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fingerprints = {}\n",
        "for class_val in [1, 2, 3]:\n",
        "    class_mask = (classification_clean == class_val)\n",
        "    fingerprints[class_val] = {\n",
        "        'spring': np.nanmean(data['NDWI']['Spring'][class_mask]),\n",
        "        'summer': np.nanmean(data['NDWI']['Summer'][class_mask]),\n",
        "        'fall': np.nanmean(data['NDWI']['Fall'][class_mask])\n",
        "    }\n",
        "\n",
        "    fp = fingerprints[class_val]\n",
        "    values = [fp['spring'], fp['summer'], fp['fall']]\n",
        "    range_val = max(values) - min(values)\n",
        "    print(f\"\\n{class_names[class_val]}:\")\n",
        "    print(f\"  Spring: {fp['spring']:+.4f}\")\n",
        "    print(f\"  Summer: {fp['summer']:+.4f}\")\n",
        "    print(f\"  Fall: {fp['fall']:+.4f}\")\n",
        "    print(f\"  Range: {range_val:.4f}\")\n",
        "\n",
        "# --- Figure 7: Seasonal Fingerprint ---\n",
        "print(\"\\nüìä Generating Figure 7: Seasonal Fingerprint...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "season_positions = [0, 1, 2]\n",
        "season_labels = ['Spring', 'Summer', 'Fall']\n",
        "\n",
        "for idx, class_val in enumerate([1, 2, 3]):\n",
        "    fp = fingerprints[class_val]\n",
        "    values = [fp['spring'], fp['summer'], fp['fall']]\n",
        "    ax.plot(season_positions, values, color=colors_lines[idx], marker=markers[idx],\n",
        "            markersize=12, linewidth=3, label=class_names_full[class_val])\n",
        "\n",
        "ax.set_xlabel('Season', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Mean NDWI (Gao)', fontsize=14, fontweight='bold')\n",
        "ax.set_title('Figure 7: Seasonal NDWI Fingerprints by Class\\n(Validation of CV Thresholds)',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "ax.axhline(0, color='black', linestyle='--', linewidth=1, label='Water/Vegetation Boundary (0.0)')\n",
        "ax.set_xticks(season_positions)\n",
        "ax.set_xticklabels(season_labels, fontsize=12)\n",
        "ax.legend(fontsize=12, loc='best', framealpha=0.95, edgecolor='black')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add interpretation text\n",
        "interp_text = ('Permanent (Blue): Stably Negative (Open Water)\\n'\n",
        "               'Vegetated (Orange): Stably Positive (Vegetation)\\n'\n",
        "               'Seasonal (Red): Fluctuating (Ephemeral)')\n",
        "ax.text(0.02, 0.98, interp_text, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='top', bbox=props)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure7_Seasonal_Fingerprint.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure7_Seasonal_Fingerprint.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: ADDITIONAL ANALYSIS FIGURES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: GENERATING ADDITIONAL ANALYSIS FIGURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Figure 6: Comprehensive Analysis (4 subplots) ---\n",
        "print(\"\\nüìä Generating Figure 6: Comprehensive Analysis...\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Subplot A: CV Distribution Histogram\n",
        "ax = axes[0, 0]\n",
        "cv_all = cv_wetlands[~np.isnan(cv_wetlands)]\n",
        "# Filter extreme values for better visualization\n",
        "cv_all_filtered = cv_all[cv_all <= 150]  # Remove extreme outliers\n",
        "\n",
        "ax.hist(cv_all_filtered, bins=80, color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
        "ax.axvline(LOW_CV_THRESHOLD, color='green', linestyle='--', linewidth=2.5,\n",
        "          label=f'Permanent Threshold (CV = {LOW_CV_THRESHOLD}%)')\n",
        "ax.axvline(HIGH_CV_THRESHOLD, color='red', linestyle='--', linewidth=2.5,\n",
        "          label=f'Seasonal Threshold (CV = {HIGH_CV_THRESHOLD}%)')\n",
        "\n",
        "ax.set_xlabel('Coefficient of Variation (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Frequency (pixels)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('(A) CV Distribution Across All Wetlands', fontsize=13, fontweight='bold', pad=10)\n",
        "ax.legend(fontsize=10, loc='upper right')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_xlim(0, 150)  # Cap at 150% for better viz\n",
        "\n",
        "# Add statistics text\n",
        "n_extreme = np.sum(cv_all > 150)\n",
        "if n_extreme > 0:\n",
        "    ax.text(0.98, 0.5, f'Note: {n_extreme:,} pixels\\nwith CV > 150%\\n(not shown)',\n",
        "           transform=ax.transAxes, fontsize=9, va='center', ha='right',\n",
        "           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "\n",
        "# Subplot B: CV by Class (Boxplot)\n",
        "ax = axes[0, 1]\n",
        "\n",
        "# Prepare data with FILTERING of extreme values\n",
        "cv_by_class = []\n",
        "class_labels = []\n",
        "for class_val in [1, 2, 3]:\n",
        "    class_cv = cv_wetlands[classification_clean == class_val]\n",
        "    class_cv = class_cv[~np.isnan(class_cv)]\n",
        "    # FILTER: Cap CV at 150% to avoid extreme outliers\n",
        "    class_cv_filtered = class_cv[class_cv <= 150]\n",
        "    cv_by_class.append(class_cv_filtered)\n",
        "    class_labels.append(class_names[class_val])\n",
        "\n",
        "    # Report filtering\n",
        "    n_filtered = len(class_cv) - len(class_cv_filtered)\n",
        "    if n_filtered > 0:\n",
        "        print(f\"  {class_names[class_val]}: Filtered {n_filtered} extreme CV values (>{150}%)\")\n",
        "\n",
        "\n",
        "# Create boxplot with better styling\n",
        "bp = ax.boxplot(cv_by_class, labels=class_labels,\n",
        "                patch_artist=True,\n",
        "                showmeans=True,\n",
        "                showfliers=True,  # Show outliers but they're now capped at 150%\n",
        "                widths=0.6,\n",
        "                medianprops=dict(color='red', linewidth=2),\n",
        "                meanprops=dict(marker='D', markerfacecolor='green', markersize=8),\n",
        "                flierprops=dict(marker='o', markerfacecolor='gray', markersize=4, alpha=0.5))\n",
        "\n",
        "# Color the boxes\n",
        "box_colors = ['lightblue', 'lightyellow', 'lightcoral']\n",
        "for patch, color in zip(bp['boxes'], box_colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_edgecolor('black')\n",
        "    patch.set_linewidth(1.5)\n",
        "\n",
        "\n",
        "ax.set_ylabel('Coefficient of Variation (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('(B) CV Distribution by Class (Filtered)', fontsize=13, fontweight='bold', pad=10)\n",
        "ax.grid(alpha=0.3, axis='y', linestyle='--')\n",
        "ax.set_ylim(0, 150)  # Set consistent y-axis\n",
        "\n",
        "# Add legend for mean/median\n",
        "from matplotlib.lines import Line2D\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], color='red', linewidth=2, label='Median'),\n",
        "    Line2D([0], [0], marker='D', color='w', markerfacecolor='green',\n",
        "           markersize=8, label='Mean')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
        "\n",
        "\n",
        "# Subplot C: Area vs Count Comparison\n",
        "ax = axes[1, 0]\n",
        "\n",
        "categories = ['Permanent', 'Vegetated', 'Seasonal']\n",
        "area_pct = [area_stats[i]['pct'] for i in [1, 2, 3]]\n",
        "patch_pct = [patch_stats[i]['num_patches']/total_patches*100 for i in [1, 2, 3]]\n",
        "\n",
        "x = np.arange(len(categories))\n",
        "width = 0.38\n",
        "\n",
        "bars1 = ax.bar(x - width/2, area_pct, width, label='% of Total Area',\n",
        "              color='steelblue', edgecolor='black', linewidth=1.5)\n",
        "bars2 = ax.bar(x + width/2, patch_pct, width, label='% of Total Patches',\n",
        "              color='coral', edgecolor='black', linewidth=1.5)\n",
        "\n",
        "ax.set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('(C) Area vs Patch Count Distribution', fontsize=13, fontweight='bold', pad=10)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories, fontsize=11, fontweight='bold')\n",
        "ax.legend(fontsize=10, loc='upper right')\n",
        "ax.grid(alpha=0.3, axis='y', linestyle='--')\n",
        "ax.set_ylim(0, 65)\n",
        "\n",
        "# Add value labels with better positioning\n",
        "for i, (a, p) in enumerate(zip(area_pct, patch_pct)):\n",
        "    ax.text(i - width/2, a + 1.5, f'{a:.1f}%', ha='center', va='bottom',\n",
        "           fontsize=10, fontweight='bold')\n",
        "    ax.text(i + width/2, p + 1.5, f'{p:.1f}%', ha='center', va='bottom',\n",
        "           fontsize=10, fontweight='bold')\n",
        "\n",
        "\n",
        "# Subplot D: Patch Size Distribution (Log scale)\n",
        "ax = axes[1, 1]\n",
        "\n",
        "all_sizes = []\n",
        "all_labels = []\n",
        "for class_val in [1, 2, 3]:\n",
        "    sizes = patch_stats[class_val]['sizes']\n",
        "    all_sizes.extend(np.log10(np.array(sizes) + 0.001))\n",
        "    all_labels.extend([class_names[class_val]] * len(sizes))\n",
        "\n",
        "sizes_df = pd.DataFrame({'Log10_Area': all_sizes, 'Class': all_labels})\n",
        "\n",
        "# Plot histograms with better binning\n",
        "for class_val, color in zip([1, 2, 3], colors_map):\n",
        "    data = sizes_df[sizes_df['Class'] == class_names[class_val]]['Log10_Area']\n",
        "    ax.hist(data, bins=45, alpha=0.6, label=class_names[class_val],\n",
        "           color=color, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "ax.set_xlabel('Log‚ÇÅ‚ÇÄ(Patch Area in ha)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Frequency (Number of Patches)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('(D) Patch Size Distribution by Class', fontsize=13, fontweight='bold', pad=10)\n",
        "ax.legend(fontsize=10, loc='upper right')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add reference lines for key sizes\n",
        "ax.axvline(np.log10(0.01), color='gray', linestyle=':', linewidth=1, alpha=0.7)\n",
        "ax.axvline(np.log10(0.1), color='gray', linestyle=':', linewidth=1, alpha=0.7)\n",
        "ax.axvline(np.log10(1.0), color='gray', linestyle=':', linewidth=1, alpha=0.7)\n",
        "\n",
        "# Add size labels\n",
        "size_labels = ['0.01 ha', '0.1 ha', '1 ha']\n",
        "size_positions = [np.log10(0.01), np.log10(0.1), np.log10(1.0)]\n",
        "for label, pos in zip(size_labels, size_positions):\n",
        "    ax.text(pos, ax.get_ylim()[1]*0.95, label, rotation=90,\n",
        "           va='top', ha='right', fontsize=8, alpha=0.7)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure6_Comprehensive_Analysis.png',\n",
        "           dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure6_Comprehensive_Analysis.png\")\n",
        "\n",
        "# --- Figure 9: Bimodal Distribution ---\n",
        "print(\"\\nüìä Generating Figure 9: Bimodal Size Distribution...\")\n",
        "all_sizes = []\n",
        "for class_val in [1, 2, 3]:\n",
        "    all_sizes.extend(patch_stats[class_val]['sizes'])\n",
        "\n",
        "all_sizes_log = np.log10(all_sizes)\n",
        "all_sizes_log = all_sizes_log[np.isfinite(all_sizes_log)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "counts, bins, patches = ax.hist(all_sizes_log, bins=50, color='#3182bd', alpha=0.8, edgecolor='black')\n",
        "ax.axvline(x=np.log10(0.03), color='red', linestyle='--', linewidth=2.5,\n",
        "          label='Peak 1: Fragmented Systems (~0.03 ha)')\n",
        "ax.axvline(x=np.log10(0.5), color='green', linestyle='--', linewidth=2.5,\n",
        "          label='Peak 2: Contiguous Systems (~0.5 ha)')\n",
        "ax.set_title('Figure 9: Bimodal Distribution of Patch Sizes (All Classes)',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Wetland Size (Log10 ha)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Frequency (Number of Patches)', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=11, loc='upper right')\n",
        "ax.grid(axis='y', alpha=0.4)\n",
        "\n",
        "# Add text annotations\n",
        "peak1_count = np.sum((all_sizes_log >= np.log10(0.01)) & (all_sizes_log < np.log10(0.1)))\n",
        "peak2_count = np.sum((all_sizes_log >= np.log10(0.3)) & (all_sizes_log < np.log10(2)))\n",
        "textstr = f'Small wetlands (<0.1 ha): {peak1_count:,}\\nLarge wetlands (>0.3 ha): {peak2_count:,}'\n",
        "ax.text(0.98, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='top', horizontalalignment='right', bbox=props, family='monospace')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure9_Bimodal_Distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure9_Bimodal_Distribution.png\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: CREATE STATISTICS TABLE AND EXPORT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: CREATING STATISTICS TABLE AND EXCEL EXPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Main statistics table\n",
        "stats_table = []\n",
        "for class_val in [1, 2, 3]:\n",
        "    if class_val == 1:\n",
        "        cv_thresh_str = f\"CV < {LOW_CV_THRESHOLD}%\"\n",
        "    elif class_val == 2:\n",
        "        cv_thresh_str = f\"{LOW_CV_THRESHOLD}%-{HIGH_CV_THRESHOLD}%\"\n",
        "    else:\n",
        "        cv_thresh_str = f\"CV > {HIGH_CV_THRESHOLD}%\"\n",
        "\n",
        "    stats_table.append({\n",
        "        'Class': class_names[class_val],\n",
        "        'CV Threshold': cv_thresh_str,\n",
        "        'Area (ha)': round(area_stats[class_val]['area_ha'], 2),\n",
        "        'Area (%)': round(area_stats[class_val]['pct'], 1),\n",
        "        'Patches': patch_stats[class_val]['num_patches'],\n",
        "        'Patch (%)': round(patch_stats[class_val]['num_patches']/total_patches*100, 1),\n",
        "        'Mean Size (ha)': round(patch_stats[class_val]['mean_size'], 4),\n",
        "        'Median Size (ha)': round(patch_stats[class_val]['median_size'], 4)\n",
        "    })\n",
        "\n",
        "df_stats = pd.DataFrame(stats_table)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL WETLAND CLASSIFICATION STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(df_stats.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Export to Excel with multiple sheets\n",
        "excel_path = f'{output_path}Wetland_Statistics_Complete.xlsx'\n",
        "print(f\"\\nüíæ Creating Excel workbook: {excel_path}\")\n",
        "\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "    # Sheet 1: Main Statistics\n",
        "    df_stats.to_excel(writer, sheet_name='Classification Statistics', index=False)\n",
        "\n",
        "    # Sheet 2: Seasonal Fingerprints\n",
        "    fingerprint_data = []\n",
        "    for season in ['spring', 'summer', 'fall']:\n",
        "        row = {'Season': season.capitalize()}\n",
        "        for class_val in [1, 2, 3]:\n",
        "            row[class_names[class_val]] = round(fingerprints[class_val][season], 4)\n",
        "        fingerprint_data.append(row)\n",
        "    df_fingerprint = pd.DataFrame(fingerprint_data)\n",
        "    df_fingerprint.to_excel(writer, sheet_name='Seasonal Fingerprints', index=False)\n",
        "\n",
        "    # Sheet 3: CV Statistics by Class\n",
        "    cv_stats_data = []\n",
        "    for class_val in [1, 2, 3]:\n",
        "        cv_stats_data.append({\n",
        "            'Class': class_names[class_val],\n",
        "            'Mean CV': round(cv_stats_by_class[class_val]['mean'], 2),\n",
        "            'Median CV': round(cv_stats_by_class[class_val]['median'], 2),\n",
        "            'Std Dev CV': round(cv_stats_by_class[class_val]['std'], 2),\n",
        "            'Min CV': round(cv_stats_by_class[class_val]['min'], 2),\n",
        "            'Max CV': round(cv_stats_by_class[class_val]['max'], 2),\n",
        "            'Q25 CV': round(cv_stats_by_class[class_val]['q25'], 2),\n",
        "            'Q75 CV': round(cv_stats_by_class[class_val]['q75'], 2)\n",
        "        })\n",
        "    df_cv_stats = pd.DataFrame(cv_stats_data)\n",
        "    df_cv_stats.to_excel(writer, sheet_name='CV Statistics', index=False)\n",
        "\n",
        "    # Sheet 4: Patch Size Statistics\n",
        "    patch_size_data = []\n",
        "    for class_val in [1, 2, 3]:\n",
        "        patch_size_data.append({\n",
        "            'Class': class_names[class_val],\n",
        "            'Number of Patches': patch_stats[class_val]['num_patches'],\n",
        "            'Mean Size (ha)': round(patch_stats[class_val]['mean_size'], 4),\n",
        "            'Median Size (ha)': round(patch_stats[class_val]['median_size'], 4),\n",
        "            'Std Dev (ha)': round(patch_stats[class_val]['std_size'], 4),\n",
        "            'Min Size (ha)': round(patch_stats[class_val]['min_size'], 4),\n",
        "            'Max Size (ha)': round(patch_stats[class_val]['max_size'], 4)\n",
        "        })\n",
        "    df_patch_size = pd.DataFrame(patch_size_data)\n",
        "    df_patch_size.to_excel(writer, sheet_name='Patch Size Statistics', index=False)\n",
        "\n",
        "    # Sheet 5: Metadata\n",
        "    metadata_info = pd.DataFrame({\n",
        "        'Parameter': [\n",
        "            'Analysis Date',\n",
        "            'Study Area',\n",
        "            'NDWI Type',\n",
        "            'Resolution',\n",
        "            'Seasons Analyzed',\n",
        "            'Total Wetland Area (ha)',\n",
        "            'Total Classified Area (ha)',\n",
        "            'Total Patches',\n",
        "            'Permanent Threshold',\n",
        "            'Seasonal Threshold',\n",
        "            'CDL Codes Used',\n",
        "            'CDL Year',\n",
        "            'Edge Buffer (pixels)',\n",
        "            'Minimum Patch Size (ha)',\n",
        "            'Patch Connectivity'\n",
        "        ],\n",
        "        'Value': [\n",
        "            datetime.now().strftime('%Y-%m-%d'),\n",
        "            'Brookings County, South Dakota',\n",
        "            'Gao NDWI (NIR-SWIR)',\n",
        "            '10m',\n",
        "            'Spring, Summer, Fall 2024',\n",
        "            f'{total_wetland_area:.2f}',\n",
        "            f'{total_classified_pixels * pixel_area_ha:.2f}',\n",
        "            total_patches,\n",
        "            f'CV < {LOW_CV_THRESHOLD}%',\n",
        "            f'CV > {HIGH_CV_THRESHOLD}%',\n",
        "            '111 (Open Water), 190 (Woody Wetlands), 195 (Herbaceous Wetlands)',\n",
        "            '2024',\n",
        "            EDGE_BUFFER_PIXELS,\n",
        "            MIN_PATCH_SIZE_HA,\n",
        "            '4-connectivity'\n",
        "        ]\n",
        "    })\n",
        "    metadata_info.to_excel(writer, sheet_name='Metadata', index=False)\n",
        "\n",
        "print(\"‚úì Excel workbook created with 5 sheets:\")\n",
        "print(\"  - Classification Statistics\")\n",
        "print(\"  - Seasonal Fingerprints\")\n",
        "print(\"  - CV Statistics\")\n",
        "print(\"  - Patch Size Statistics\")\n",
        "print(\"  - Metadata\")\n",
        "\n",
        "# Save CSV version too\n",
        "csv_path = f'{output_path}Wetland_Statistics_Summary.csv'\n",
        "df_stats.to_csv(csv_path, index=False)\n",
        "print(f\"‚úì CSV summary saved: {csv_path}\")\n",
        "\n",
        "# --- Generate Statistics Table Figure ---\n",
        "print(\"\\nüìä Generating Statistics Table Figure...\")\n",
        "fig, ax = plt.subplots(figsize=(16, 5))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "table_data = [df_stats.columns.tolist()] + df_stats.values.tolist()\n",
        "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                colWidths=[0.20, 0.14, 0.10, 0.09, 0.10, 0.09, 0.14, 0.14])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2.2)\n",
        "\n",
        "# Style header row\n",
        "for i in range(len(df_stats.columns)):\n",
        "    table[(0, i)].set_facecolor('#08519c')\n",
        "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "# Style data rows\n",
        "colors_rows = ['#c6dbef', '#fee0d2', '#fcbba1']\n",
        "for i in range(1, len(table_data)):\n",
        "    for j in range(len(df_stats.columns)):\n",
        "        table[(i, j)].set_facecolor(colors_rows[i-1])\n",
        "        table[(i, j)].set_text_props(weight='bold')\n",
        "\n",
        "ax.set_title('Wetland Classification Statistics Summary\\nBrookings County, SD - 2024',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_path}Figure_Statistics_Table.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.close(fig)\n",
        "print(\"‚úì Saved: Figure_Statistics_Table.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: FINAL SUMMARY AND VALIDATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä KEY FINDINGS:\")\n",
        "print(f\"  Total wetland area analyzed: {total_wetland_area:.2f} ha\")\n",
        "print(f\"  Total classified area: {total_classified_pixels * pixel_area_ha:.2f} ha\")\n",
        "print(f\"  Total number of patches: {total_patches:,}\")\n",
        "print(f\"\\n  Dominant class by AREA: {class_names[max(area_stats, key=lambda x: area_stats[x]['pct'])]} ({max([area_stats[i]['pct'] for i in [1,2,3]]):.1f}%)\")\n",
        "print(f\"  Dominant class by COUNT: {class_names[max(patch_stats, key=lambda x: patch_stats[x]['num_patches'])]} ({max([patch_stats[i]['num_patches']/total_patches*100 for i in [1,2,3]]):.1f}%)\")\n",
        "\n",
        "print(\"\\n‚úÖ DATA QUALITY CHECKS:\")\n",
        "# Check 1: Total percentages\n",
        "total_area_pct = sum([area_stats[i]['pct'] for i in [1, 2, 3]])\n",
        "total_patch_pct = sum([patch_stats[i]['num_patches']/total_patches*100 for i in [1, 2, 3]])\n",
        "print(f\"  Area percentages sum: {total_area_pct:.1f}% {'‚úì' if abs(total_area_pct - 100) < 0.5 else '‚ö†Ô∏è'}\")\n",
        "print(f\"  Patch percentages sum: {total_patch_pct:.1f}% {'‚úì' if abs(total_patch_pct - 100) < 0.5 else '‚ö†Ô∏è'}\")\n",
        "\n",
        "# Check 2: NDWI fingerprint validation\n",
        "for class_val in [1, 2, 3]:\n",
        "    fp = fingerprints[class_val]\n",
        "    values = [fp['spring'], fp['summer'], fp['fall']]\n",
        "    range_val = max(values) - min(values)\n",
        "    mean_val = np.mean(values)\n",
        "\n",
        "    if class_val == 1:  # Permanent should be negative and stable\n",
        "        check = mean_val < -0.1 and range_val < 0.1\n",
        "        print(f\"  {class_names[class_val]:20s}: Mean NDWI = {mean_val:+.3f}, Range = {range_val:.3f} {'‚úì' if check else '‚ö†Ô∏è'}\")\n",
        "    elif class_val == 3:  # Seasonal should fluctuate\n",
        "        check = range_val > 0.2\n",
        "        print(f\"  {class_names[class_val]:20s}: Mean NDWI = {mean_val:+.3f}, Range = {range_val:.3f} {'‚úì' if check else '‚ö†Ô∏è'}\")\n",
        "    else:  # Vegetated\n",
        "        check = mean_val > 0\n",
        "        print(f\"  {class_names[class_val]:20s}: Mean NDWI = {mean_val:+.3f}, Range = {range_val:.3f} {'‚úì' if check else '‚ö†Ô∏è'}\")\n",
        "\n",
        "print(\"\\nüìÅ OUTPUT FILES GENERATED:\")\n",
        "output_files = [\n",
        "    \"Figure1_Wetland_Mask.png\",\n",
        "    \"Figure2_CV_Map.png\",\n",
        "    \"Figure3_Classification_Map.png\",\n",
        "    \"Figure4_Pie_Charts.png\",\n",
        "    \"Figure6_Comprehensive_Analysis.png\",\n",
        "    \"Figure7_Seasonal_Fingerprint.png\",\n",
        "    \"Figure9_Bimodal_Distribution.png\",\n",
        "    \"Figure_Statistics_Table.png\",\n",
        "    \"Wetland_Mask.tif\", # Added Wetland Mask GeoTIFF\n",
        "    \"CV_Map.tif\",\n",
        "    \"Wetland_Classification.tif\",\n",
        "    \"Wetland_Statistics_Complete.xlsx\",\n",
        "    \"Wetland_Statistics_Summary.csv\"\n",
        "]\n",
        "\n",
        "for filename in output_files:\n",
        "    filepath = f\"{output_path}{filename}\"\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"  ‚úì {filename:40s} ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  ‚úó {filename:40s} (NOT FOUND)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nAll outputs saved to: {output_path}\")\n",
        "print(f\"Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\\nüéâ Your wetland classification is ready for report writing!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}